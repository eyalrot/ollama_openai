# Test Configuration
# The tests will connect to the ollama_openai proxy on port 11434
# which forwards to LiteLLM on port 4000

# Ollama-OpenAI Proxy Configuration (default values)
# PROXY_HOST=http://localhost:11434
# OPENAI_ENDPOINT=http://localhost:11434/v1

# Optional: Log level for tests
LOG_LEVEL=INFO

# Optional: Custom test parameters
# TIMEOUT=30
# MAX_RETRIES=3