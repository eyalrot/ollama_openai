# Ollama to OpenAI Proxy Configuration
# =====================================

# Required Configuration
# ======================

# Target OpenAI-compatible server URL (must end with /v1)
# Examples:
# - VLLM: http://vllm-server:8000/v1
# - OpenRouter: https://openrouter.ai/api/v1
# - Local LLM: http://localhost:8000/v1
OPENAI_API_BASE_URL=http://your-openai-server:8000/v1

# Authentication key for the target server
# For OpenRouter, get your key from: https://openrouter.ai/keys
OPENAI_API_KEY=your-api-key-here

# Optional Configuration
# ======================

# Proxy server port (default: 11434 - Ollama's standard port)
PROXY_PORT=11434

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO)
LOG_LEVEL=INFO

# Request timeout in seconds (default: 60)
REQUEST_TIMEOUT=60

# Maximum retry attempts for failed requests (default: 3)
MAX_RETRIES=3

# Path to optional model name mapping JSON file
# Use this to map Ollama model names to OpenAI model names
# Example: {"llama2": "meta-llama/llama-2-7b-chat", "codellama": "codellama/CodeLlama-7b"}
MODEL_MAPPING_FILE=

# Enable debug mode for development (default: false)
DEBUG=false

# Testing Configuration (OpenRouter)
# ==================================
# For testing with OpenRouter's free models, use:
# OPENAI_API_BASE_URL=https://openrouter.ai/api/v1
# OPENAI_API_KEY=your-openrouter-api-key
#
# Free models available for testing:
# - google/gemma-2-9b-it:free
# - meta-llama/llama-3.2-3b-instruct:free
# - microsoft/phi-3-mini-128k-instruct:free
# - qwen/qwen-2.5-7b-instruct:free