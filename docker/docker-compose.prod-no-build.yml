services:
  ollama-proxy:
    image: eyalrot2/ollama-openai-proxy:prod
    ports:
      - "${PROXY_PORT:-11434}:11434"
    environment:
      - OPENAI_API_BASE_URL=${OPENAI_API_BASE_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - PROXY_PORT=${PROXY_PORT:-11434}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT:-60}
      - MAX_RETRIES=${MAX_RETRIES:-3}
      - MODEL_MAPPING_FILE=${MODEL_MAPPING_FILE:-}
      - DEBUG=false
    volumes:
      # Mount config directory for model mappings (read-only)
      - ../config:/app/config:ro
      # Mount logs directory for persistent logging
      - ../logs:/app/logs
    restart: always
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"
        labels: "service=ollama-proxy-no-build"
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp