services:
  ollama-proxy-dev:
    build:
      context: ..
      dockerfile: docker/Dockerfile.dev
    image: ollama-openai-proxy:dev
    container_name: ollama-proxy-dev
    ports:
      - "${PROXY_PORT:-11434}:11434"
    environment:
      - OPENAI_API_BASE_URL=${OPENAI_API_BASE_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - PROXY_PORT=${PROXY_PORT:-11434}
      - LOG_LEVEL=${LOG_LEVEL:-DEBUG}
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT:-60}
      - MAX_RETRIES=${MAX_RETRIES:-3}
      - MODEL_MAPPING_FILE=${MODEL_MAPPING_FILE:-}
      - DEBUG=true
      - DISABLE_SSL_VERIFICATION=${DISABLE_SSL_VERIFICATION:-false}
    volumes:
      # Mount source code for hot-reload
      - ../src:/app/src
      - ../tests:/app/tests
      # Mount config directory
      - ../config:/app/config
      # Mount logs directory
      - ../logs:/app/logs
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - ollama-network-dev
    # Development overrides
    stdin_open: true
    tty: true

networks:
  ollama-network-dev:
    driver: bridge